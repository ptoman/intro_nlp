{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Machine learning on words\n",
    "\n",
    "In this notebook we'll assign better weights to the words we identified earlier -- weights that are actually optimal for classifying the data on our training set.  We'll have to figure out how to represent words as numbers to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_splits.p\", \"rb\") as f:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the set of words we're interested in\n",
    "words_of_interest = {}\n",
    "with open(\"data/hand_weights.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for word, score in reader:\n",
    "        words_of_interest[word] = 1\n",
    "words_of_interest = words_of_interest.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check out the words we'll learn information about\n",
    "print words_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a function that will convert each paragraph to a vector.\n",
    "#\n",
    "# The presence of each word in the wordlist is a feature.\n",
    "# So a cell is 1 if the word of interest appears in the\n",
    "# paragraph, and near 0 otherwise\n",
    "def convert_to_vector(paragraph):\n",
    "    representation = np.zeros(len(words_of_interest))\n",
    "    for i, word in enumerate(words_of_interest):\n",
    "        if word in paragraph:\n",
    "            representation[i] = 1\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the conversion\n",
    "print convert_to_vector(\"not bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_dataset(dataset):\n",
    "    # Convert X_train and X_dev to use the new format\n",
    "    dataset_vector = np.zeros((len(dataset), len(words_of_interest)))\n",
    "    for i,paragraph in enumerate(dataset):\n",
    "        dataset_vector[i] = convert_to_vector(paragraph)\n",
    "    return dataset_vector\n",
    "\n",
    "X_train_vector = convert_dataset(X_train)\n",
    "print X_train_vector.shape\n",
    "X_dev_vector = convert_dataset(X_dev)\n",
    "print X_dev_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train_vector, y_train)\n",
    "y_dev_hat = clf.predict(X_dev_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate\n",
    "# No cross-validation this round, but we can use that in the \n",
    "# future to get a sense of the variability of the method\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy:\"\n",
    "print metrics.accuracy_score(y_dev, y_dev_hat)\n",
    "\n",
    "print\n",
    "\n",
    "print \"Classification metrics:\"\n",
    "print metrics.classification_report(y_dev, y_dev_hat)\n",
    "\n",
    "print \n",
    "\n",
    "print \"Confusion matrix:\"\n",
    "print \"(Rows are truth, columns are predictions)\"\n",
    "print metrics.confusion_matrix(y_dev, y_dev_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance went up!  That's good, and what we'd expect.\n",
    "\n",
    "Let's brainstorm some possible reasons why it isn't great yet, though:\n",
    "- Too much regularization\n",
    "- Overfit to the training data\n",
    "- Too little data, such that what we're learning is noisy\n",
    "- Too many rows have 0 occurrences of one of our training words\n",
    "\n",
    "It turns out there isn't too much regularization -- you can play with this by passing C=2.0 or higher numbers to the LogisticRegression creator.  Regardless of what this value is, performance doesn't improve.\n",
    "\n",
    "Let's check out whether we're overfitting or underfitting to the problem next..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "param = \"C\"\n",
    "param_range = [0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0] #[True, False] #\n",
    "train_scores, test_scores = validation_curve(\n",
    "    linear_model.LogisticRegression(), \n",
    "    X_train_vector, \n",
    "    y_train, \n",
    "    cv=10, \n",
    "    param_name=param, \n",
    "    param_range=param_range, \n",
    "    scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "print train_mean\n",
    "print train_std\n",
    "print test_mean\n",
    "print test_std\n",
    "\n",
    "plt.title(\"Validation  Curve\")\n",
    "plt.xlabel(param)\n",
    "plt.ylabel(\"F1\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.semilogx(param_range,\n",
    "             train_mean, \n",
    "             label=\"Training score\", \n",
    "             color=\"r\")\n",
    "plt.fill_between(param_range,\n",
    "            train_mean - train_std,\n",
    "            train_mean + train_std,\n",
    "            alpha=0.2,\n",
    "            color=\"r\")\n",
    "plt.semilogx(param_range,\n",
    "             test_mean, \n",
    "             label=\"Crossvalidation score\", \n",
    "             color=\"g\")\n",
    "plt.fill_between(param_range,\n",
    "            test_mean - test_std,\n",
    "            test_mean + test_std,\n",
    "            alpha=0.2,\n",
    "            color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty great -- we're not overfitting or underfitting much at all, and the amount of regularization is reasonably good where it is (though we might want to decrease it a bit and improve performance).  \n",
    "\n",
    "Overall, this is great news. We can get a 5% gain over just guessing by using the small number of words we thought a priori might matter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
