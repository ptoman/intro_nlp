{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (term frequency-inverse document frequency)\n",
    "\n",
    "In this notebook we'll learn about TF-IDF by trying to figure out if there are some words that are indicative of positive or negative sentiment.\n",
    "\n",
    "Usually TF-IDF is used to figure out how sets of words differ from each other. It allows you to find words that occur very frequently in one document class compared to another with two simple insights:\n",
    "* Term frequency matters. Terms that are more frequent in a document are more indicative of the topic of that document. (\"Term-Frequency\")\n",
    "* Some words are very frequent but meaningless for distinguishing documents (e.g., 'the').  These terms will tend to be frequent across many documents. (\"Inverse Document Frequency\")\n",
    "\n",
    "We multiply a function of the term frequency with a function of the inverse document frequency to get the relative weight of each word as an indicator of each document class.\n",
    "\n",
    "Mostly this notebook is an excuse to show some code for doing TF-IDF -- it doesn't really make sense to have just 2 document classes and use TF-IDF.\n",
    "\n",
    "We often use TF-IDF weights instead of raw frequencies or presence/absence data to appropriately scale the importance of each word in describing a document.  It is a very old metric, not based on much math, but it works surprisingly well in practice.\n",
    "\n",
    "(You could imagine a weighted average of dense vectors for a sentence based on TF-IDF weights from a training corpus... Does that work better or worse than a straight average?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_splits.p\", \"rb\") as f:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to precalculate words of interest because this is\n",
    "# a tiny dataset... but as the dataset grows, extra passes through it are \n",
    "# a bad idea\n",
    "words_of_interest = Counter() # number of documents each word occurs in\n",
    "for item in X_train:\n",
    "    for word in set(item.split()):\n",
    "        words_of_interest[word] += 1\n",
    "\n",
    "print \"Number of unique words:\"\n",
    "print len(words_of_interest)\n",
    "print \"Number of words that occur in more than 1 document:\"\n",
    "print len([val for val, amt in words_of_interest.items() if amt > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a mapping from each word in the vocabulary to its \n",
    "# index in our one-hot embedding space.\n",
    "# Any words that occur only once are mapped to the special token\n",
    "# \"UNK\" -- others are mapped according to their frequency\n",
    "embeddings = {}\n",
    "UNK_IDX = 0\n",
    "known_words_in_vocab = 1\n",
    "for word, count in words_of_interest.most_common():\n",
    "    if count == 1:\n",
    "        embeddings[word] = UNK_IDX\n",
    "    else:\n",
    "        embeddings[word] = known_words_in_vocab\n",
    "        known_words_in_vocab += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function that will convert each paragraph to a vector.\n",
    "def convert_to_vector(paragraph):\n",
    "    representation = np.zeros(known_words_in_vocab)\n",
    "    for word in paragraph.split():\n",
    "        if word in embeddings:\n",
    "            idx = embeddings[word]\n",
    "            representation[idx] = 1\n",
    "        else:\n",
    "            representation[UNK_IDX] = 1\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a matrix with 2 document classes: positive reviews and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, targets):\n",
    "    # Create a 2 row dataset (neg/pos) with columns\n",
    "    dataset_vector = np.zeros((2, known_words_in_vocab))\n",
    "    for i,paragraph in enumerate(dataset):\n",
    "        row = 0 if targets[i] == -1 else 1\n",
    "        dataset_vector[row] += convert_to_vector(paragraph)\n",
    "    return dataset_vector\n",
    "\n",
    "X_train_vector = create_dataset(X_train, y_train)\n",
    "print X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see the relative frequencies of the top 20 words:\n",
    "\n",
    "most_common = words_of_interest.most_common(20)\n",
    "\n",
    "print \"%-10s %5s %5s\" % (\"Word\", \"Neg\", \"Pos\")\n",
    "for i in range(20):\n",
    "    print \"%-10s %5d %5d\" % (most_common[i][0], X_train_vector[0,i], X_train_vector[1,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see some potentially interesting things already...\n",
    "* \"and\" is positive\n",
    "* \"this\" is positive\n",
    "* \"but\" is negative\n",
    "\n",
    "But it's unclear whether we might be reading things in here, so let's use TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X_train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf.toarray()   \n",
    "print tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's look at the weights of the mosts common words\n",
    "most_common = words_of_interest.most_common(20)\n",
    "\n",
    "print \"%-10s %5s %5s\" % (\"Word\", \"Neg\", \"Pos\")\n",
    "for i in range(20):\n",
    "    print \"%-10s %5.2f %5.2f\" % (most_common[i][0], tfidf[0,i], tfidf[1,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tfidf.shape\n",
    "print np.argmax(tfidf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = tfidf.argpartition(-20)[:,-20:]\n",
    "most_common = words_of_interest.most_common(100)\n",
    "\n",
    "def get_words(idx):\n",
    "    words = [(most_common[i][0], most_common[i][1], tfidf[idx,i]) for i in indices[idx]]\n",
    "    words = sorted(words, key=lambda x: x[2], reverse=True)\n",
    "    for unit in words:\n",
    "        print \"%-10s %5d %5.2f\" % unit\n",
    "    return words\n",
    "\n",
    "print \"Negative words:\"\n",
    "negwords = get_words(0)\n",
    "print\n",
    "print \"Positive words:\"\n",
    "poswords = get_words(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So -- unsurprisingly since we only have 2 classes -- TF-IDF is really not that helpful here. If we had more classes we may find more interesting things.  This makes sense, of course, since \"inverse document frequency\" with only two classes is not very helpful -- a word can occur in either 1 or 2 different documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
