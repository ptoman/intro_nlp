{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hand engineered features\n",
    "\n",
    "In this notebook we'll hand-engineer some word features and score our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_splits.p\", \"rb\") as f:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's frame this problem as averaging word scores across a sentence. Each word can get a score between 1 and -1.\n",
    "\n",
    "We might start out by generating this list:\n",
    "- good, 1.0\n",
    "- enjoyable, 1.0\n",
    "- interesting, 0\n",
    "- bad, -1.0\n",
    "- really, 0\n",
    "- very, 0\n",
    "- boring, -1\n",
    "\n",
    "We might also want to include phrases, like:\n",
    "- edge of my seat, 1\n",
    "- couldn't stop eating my popcorn, 1\n",
    "\n",
    "For this exercise, we'll stick with single words (\"unigrams\") for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out which words to include from nothing is hard!\n",
    "\n",
    "Let's take a look at some training data to help us along...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = zip(X_train, y_train)\n",
    "pos = [x for x in X if x[1] == 1]\n",
    "neg = [x for x in X if x[1] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Positive first...\n",
    "for item in pos[0:5]:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep:\n",
    "- nice, 0.75\n",
    "- marvelous, 1\n",
    "- courage, 0.75\n",
    "- emotional, 0.75\n",
    "\n",
    "Do you see anything else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now negative...\n",
    "for item in neg[0:5]:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep:\n",
    "- somber, -0.25\n",
    "- little, -0.1\n",
    "- funny, 1\n",
    "- extraordinarily, 0.8\n",
    "- beautiful, 1\n",
    "- wonderful, 1\n",
    "- weird, -1\n",
    "\n",
    "This is clearly time-consuming. It's hard to list all the variants of the same word -- though this can be helped with stemming. It's also highly driven by personal judgments that are hard to justify.\n",
    "\n",
    "Within the problem itself, lots of the meaning seems to be compositional rather than word-level.  It's also not always clear from the paragraph itself, out of context, whether a review is marked positive or negative.\n",
    "\n",
    "These scores and some others have been saved in data/hand_weights.csv.  Feel free to look through the data and/or to add your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's pull in the weights\n",
    "weights = {}\n",
    "with open(\"data/hand_weights.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for word, score in reader:\n",
    "        weights[word] = float(score)\n",
    "        \n",
    "# And check them\n",
    "print \"Number of words:\", len(weights)\n",
    "print\n",
    "print \"Sample of words:\"\n",
    "for k in weights.keys()[0:10]:\n",
    "    print k, weights[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's predict\n",
    "y_dev_hat = []\n",
    "\n",
    "for item in X_dev:\n",
    "    score = 0\n",
    "    for word in item:\n",
    "        if word in weights:\n",
    "            # Use the score if we have it\n",
    "            score += weights[word]\n",
    "        else:\n",
    "            # Use a small random number centered on 0 if not\n",
    "            # (this helps ensure we always get a positive or\n",
    "            # negative # and shouldn't hurt any real information)\n",
    "            score += random.uniform(-0.001, 0.001)\n",
    "    # Convert score to an assessment\n",
    "    avg_score = score/len(item)\n",
    "    y_dev_hat.append((avg_score > 0)*2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate\n",
    "# No cross-validation this round, but we can use that in the \n",
    "# future to get a sense of the variability of the method\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy:\"\n",
    "print metrics.accuracy_score(y_dev, y_dev_hat)\n",
    "\n",
    "print\n",
    "\n",
    "print \"Classification metrics:\"\n",
    "print metrics.classification_report(y_dev, y_dev_hat)\n",
    "\n",
    "print \n",
    "\n",
    "print \"Confusion matrix:\"\n",
    "print \"(Rows are truth, columns are predictions)\"\n",
    "print metrics.confusion_matrix(y_dev, y_dev_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is pretty lousy -- if we put on rose-colored glasses, we might be slightly above chance.  We can't be sure, of course, unless we sample repeatedly (but why bother -- with performance this bad, this method is useless).\n",
    "\n",
    "You can probably image a host of ways to improve this method.\n",
    "\n",
    "We'll investigate more rigorous, data-driven methods going forward, but others you're welcome to experiment with on your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
