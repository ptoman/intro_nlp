{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Machine learning with smarter features\n",
    "\n",
    "In this notebook we'll augment the unigram model, often called a \"bag of words\" model, with a handful of smarter features.  We'll start doing *feature engineering*.  \n",
    "\n",
    "You should be able to add some features of your own that might also improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_splits.p\", \"rb\") as f:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get the set of words that occur in at least 2 documents\n",
    "words_of_interest = Counter()\n",
    "for item in X_train:\n",
    "    for word in set(item.split()):\n",
    "        words_of_interest[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's build a feature function that prepends all words\n",
    "# that between \"not\" and punctuation with \"_NOT\",\n",
    "# as per:\n",
    "# Das, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).\n",
    "# Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.  2002.  Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP-2002, 79â€”86.\n",
    "#\n",
    "# This could be much more complicated -- we could use parsing, \n",
    "# for instance -- so that's something to play with if you'd like.\n",
    "PUNCTUATION = string.punctuation + \"--\"\n",
    "KNOWN_WORDS = dict([(val, amt) for val, amt in words_of_interest.items() if amt > 1])\n",
    "def get_notted_unigrams(paragraph, verbose=False):    \n",
    "    preceded_by_not = False\n",
    "    \n",
    "    unigrams = paragraph.split()\n",
    "    for i, word in enumerate(unigrams):\n",
    "        if word not in KNOWN_WORDS:\n",
    "            unigrams[i] = word = \"UNK\"\n",
    "        \n",
    "        if word in PUNCTUATION:\n",
    "            preceded_by_not = False\n",
    "        elif preceded_by_not:\n",
    "            unigrams[i] = \"NOT_\"+word\n",
    "        elif word == \"not\":\n",
    "            preceded_by_not = True\n",
    "    if verbose:\n",
    "        print unigrams\n",
    "    return Counter(unigrams)\n",
    "\n",
    "print \"Samples:\"\n",
    "get_notted_unigrams(\"the abject big brown bear is not happy .\", True)\n",
    "get_notted_unigrams(\"it was not terrible and not awful ; in fact , it was quite good .\", True)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This time we leverage the DictVectorizer to get features --\n",
    "# we walk across the entirety of the training data to get a list of\n",
    "# feature vectors, and then we create a matrix using that approach.\n",
    "X_train_feat_dicts = []\n",
    "for item in X_train:\n",
    "    X_train_feat_dicts.append(get_notted_unigrams(item))\n",
    "\n",
    "# Create a vectorizer from the dictionary\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_train_matrix = vectorizer.fit_transform(X_train_feat_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the same vectorizer to transform the dev data\n",
    "X_dev_feat_dicts = []\n",
    "for item in X_dev:\n",
    "    X_dev_feat_dicts.append(get_notted_unigrams(item))\n",
    "\n",
    "X_dev_matrix = vectorizer.transform(X_dev_feat_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train_matrix, y_train)\n",
    "y_dev_hat = clf.predict(X_dev_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate\n",
    "# No cross-validation this round, but we can use that in the \n",
    "# future to get a sense of the variability of the method\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy:\"\n",
    "print metrics.accuracy_score(y_dev, y_dev_hat)\n",
    "\n",
    "print\n",
    "\n",
    "print \"Classification metrics:\"\n",
    "print metrics.classification_report(y_dev, y_dev_hat)\n",
    "\n",
    "print \n",
    "\n",
    "print \"Confusion matrix:\"\n",
    "print \"(Rows are truth, columns are predictions)\"\n",
    "print metrics.confusion_matrix(y_dev, y_dev_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we experienced basically no effect by incorporating this feature. In particular, we see that recall goes up for the negative cases, and precision goes up for the positive cases -- this suggests that we are capturing negativity better now.  This is what we were going for, so it's good news!\n",
    "\n",
    "But what might be nice is to get a sense of the statistical range of what's going on here.  So let's switch to cross-validation and do a model comparison of the \"_NOT\" augmented model from the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross-validation on this model\n",
    "from sklearn import cross_validation\n",
    "scores = cross_validation.cross_val_score(clf, X_train_matrix, y_train, cv=10 )\n",
    "\n",
    "print \"The 95% confidence interval on the model performance is...\"\n",
    "print \"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(scores), 2*np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that the new model performs statistically similar on this training corpus.  However, since now we're able to differentiate \"good\" and \"not good\", we have a theoretical reason to prefer the current model.\n",
    "\n",
    "To further improve performance, you might want to introduce other feature functions, or improvements to the current feature function (like stopping at phrase boundaries instead of punctuation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
