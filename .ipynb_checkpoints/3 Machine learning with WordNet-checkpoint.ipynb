{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine learning with WordNet\n",
    "\n",
    "In this notebook we'll grow the dataset to include the synonyms of the words we initially identified.  To grow the dataset, we will use WordNet. WordNet is like a megathesaurus. It groups words into \"synsets\", and provides relations like antonymy and metonymy between word pairs. It also has information about stems.  It covers all parts of speech.\n",
    "\n",
    "This is an oldie but goodie resource. Sometimes you'll want something that WordNet puts at your fingertips.\n",
    "\n",
    "https://wordnet.princeton.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_splits.p\", \"rb\") as f:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the set of words we're interested in from the text document\n",
    "words_of_interest = {}\n",
    "with open(\"data/hand_weights.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for word, score in reader:\n",
    "        words_of_interest[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demonstrate WordNet senses:\n",
    "print wn.synsets('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are all these senses, you ask?  There is an online tool:\n",
    "# http://wordnetweb.princeton.edu/perl/webwn\n",
    "\n",
    "# You can also look in more depth...\n",
    "# Look for only a certain part of speech\n",
    "print wn.synsets('cat', pos=wn.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at definitions\n",
    "print wn.synsets('cat', pos=wn.VERB)[0].definition()\n",
    "print wn.synsets('cat', pos=wn.VERB)[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at examples\n",
    "print wn.synsets('cat', pos=wn.VERB)[1].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at lemmas. Lemmas (surface manifestations of particular orderings of characters) have senses.\n",
    "print wn.synsets('cat', pos=wn.VERB)[1].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also access a synset more directly using its surface form + part of speech + sense number\n",
    "print wn.synset('cat.v.1').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WordNet is all about word *senses*, so it wants:\n",
    "# (a) the part of speech, and (b) which sense you are interested in\n",
    "# These are hard to get from raw text! So what we are going to do\n",
    "# is assume the part of speech is always adjective (which is on\n",
    "# the whole true -- but fails for words like w00t), and we are going\n",
    "# to always choose the most common sense of a word, which is listed\n",
    "# first.\n",
    "# In real texts, you can use part-of-speech tagging preprocessing\n",
    "# to get more information as to the right sense to guess.\n",
    "\n",
    "# Let's see how well the first sense does on the first 10 words,\n",
    "# assuming adjective status\n",
    "for word in words_of_interest.keys()[0:10]:\n",
    "    print \"%-10s %-30s\" % (word, wn.synsets(word, wn.ADJ))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's build out our words of interest\n",
    "expanded_adjectives = set(words_of_interest.keys())\n",
    "for word in words_of_interest.keys():\n",
    "    synsets = wn.synsets(word, wn.ADJ)\n",
    "    if len(synsets) > 0:\n",
    "        for lemma in synsets[0].lemmas():\n",
    "            expanded_adjectives.add(lemma.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How much did our set change?\n",
    "print \"The original list:\"\n",
    "print len(words_of_interest)\n",
    "print \"The expanded list:\"\n",
    "print len(expanded_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"New words that we missed on the first pass:\"\n",
    "print expanded_adjectives - set(words_of_interest.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a function that will convert each paragraph to a vector.\n",
    "#\n",
    "# The presence of each word in the wordlist is a feature.\n",
    "# So a cell is 1 if the word of interest appears in the\n",
    "# paragraph, and near 0 otherwise\n",
    "def convert_to_vector(paragraph):\n",
    "    representation = np.zeros(len(expanded_adjectives))\n",
    "    for i, word in enumerate(expanded_adjectives):\n",
    "        if word in paragraph.decode('latin-1'):\n",
    "            representation[i] = 1\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_dataset(dataset):\n",
    "    # Convert X_train and X_dev to use the new format\n",
    "    dataset_vector = np.zeros((len(dataset), len(expanded_adjectives)))\n",
    "    for i,paragraph in enumerate(dataset):\n",
    "        dataset_vector[i] = convert_to_vector(paragraph)\n",
    "    return dataset_vector\n",
    "\n",
    "X_train_vector = convert_dataset(X_train)\n",
    "print X_train_vector.shape\n",
    "X_dev_vector = convert_dataset(X_dev)\n",
    "print X_dev_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train_vector, y_train)\n",
    "y_dev_hat = clf.predict(X_dev_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate\n",
    "# No cross-validation this round, but we can use that in the \n",
    "# future to get a sense of the variability of the method\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy:\"\n",
    "print metrics.accuracy_score(y_dev, y_dev_hat)\n",
    "\n",
    "print\n",
    "\n",
    "print \"Classification metrics:\"\n",
    "print metrics.classification_report(y_dev, y_dev_hat)\n",
    "\n",
    "print \n",
    "\n",
    "print \"Confusion matrix:\"\n",
    "print \"(Rows are truth, columns are predictions)\"\n",
    "print metrics.confusion_matrix(y_dev, y_dev_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could easily imagine using all the synsets instead of just the primary sense of a word.  If you try that, does performance go up?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
