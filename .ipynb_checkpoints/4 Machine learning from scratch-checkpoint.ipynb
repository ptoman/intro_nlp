{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine learning on words\n",
    "\n",
    "In this notebook we'll learn both the weights and the vocabulary.  Using these \"unigrams\" is very common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_splits.p\", \"rb\") as f:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We're going to precalculate words of interest because this is\n",
    "# a tiny dataset... but as the dataset grows, extra passes through it are \n",
    "# a bad idea\n",
    "words_of_interest = Counter() # number of documents each word occurs in\n",
    "for item in X_train:\n",
    "    for word in set(item.split()):\n",
    "        words_of_interest[word] += 1\n",
    "\n",
    "print \"Number of unique words:\"\n",
    "print len(words_of_interest)\n",
    "print \"Number of words that occur in more than 1 document:\"\n",
    "print len([val for val, amt in words_of_interest.items() if amt > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's create a mapping from each word in the vocabulary to its \n",
    "# index in our one-hot embedding space.\n",
    "# Any words that occur only once are mapped to the special token\n",
    "# \"UNK\" -- others are mapped according to their frequency\n",
    "embeddings = {}\n",
    "UNK_IDX = 0\n",
    "known_words_in_vocab = 1\n",
    "for word, count in words_of_interest.most_common():\n",
    "    if count == 1:\n",
    "        embeddings[word] = UNK_IDX\n",
    "    else:\n",
    "        embeddings[word] = known_words_in_vocab\n",
    "        known_words_in_vocab += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a function that will convert each paragraph to a vector.\n",
    "def convert_to_vector(paragraph):\n",
    "    representation = np.zeros(known_words_in_vocab)\n",
    "    for word in paragraph.split():\n",
    "        if word in embeddings:\n",
    "            idx = embeddings[word]\n",
    "            representation[idx] = 1\n",
    "        else:\n",
    "            representation[UNK_IDX] = 1\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the embedding\n",
    "convert_to_vector(\"The comedy by Voltaire was hysterical .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_dataset(dataset):\n",
    "    # Convert X_train and X_dev to use the new format\n",
    "    dataset_vector = np.zeros((len(dataset), known_words_in_vocab))\n",
    "    for i,paragraph in enumerate(dataset):\n",
    "        dataset_vector[i] = convert_to_vector(paragraph)\n",
    "    return dataset_vector\n",
    "\n",
    "X_train_vector = convert_dataset(X_train)\n",
    "print X_train_vector.shape\n",
    "X_dev_vector = convert_dataset(X_dev)\n",
    "print X_dev_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train_vector, y_train)\n",
    "y_dev_hat = clf.predict(X_dev_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's evaluate\n",
    "# No cross-validation this round, but we can use that in the \n",
    "# future to get a sense of the variability of the method\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy:\"\n",
    "print metrics.accuracy_score(y_dev, y_dev_hat)\n",
    "\n",
    "print\n",
    "\n",
    "print \"Classification metrics:\"\n",
    "print metrics.classification_report(y_dev, y_dev_hat)\n",
    "\n",
    "print \n",
    "\n",
    "print \"Confusion matrix:\"\n",
    "print \"(Rows are truth, columns are predictions)\"\n",
    "print metrics.confusion_matrix(y_dev, y_dev_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, look how much better we're doing using only unigram features!  We get more than 20% gain by using whatever words happen to be in the training set, and we're doing almost no work to get this performance.\n",
    "\n",
    "Clearly our brains are not so good at coming up with rules compared to leveraging data..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
