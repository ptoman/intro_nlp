{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Machine learning with feature extraction on GloVe\n",
    "\n",
    "In this notebook we'll combine feature extraction with GloVe vectors to see if we can get a model that performs better than simple averaging of the meaning space of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import cosine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/sentiment_splits.p\", \"rb\") as f:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_DIM = 50 #using more dimensions should be better accuracy\n",
    "PATH_TO_GLOVE = \"../glove.6B/\" \n",
    "\n",
    "path_name = PATH_TO_GLOVE + \"glove.6B.{0}d.txt\".format(NUM_DIM) \n",
    "reader = csv.reader(open(path_name), delimiter=' ', quoting=csv.QUOTE_NONE)    \n",
    "glove = {line[0]: np.array(list(map(float, line[1: ]))) for line in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's identify a centroid within the GloVe vector space for\n",
    "# positive and negative words.\n",
    "#\n",
    "# There are many seed sets available, or you could create your own\n",
    "# from scratch or from crawling synonyms in a tool like WordNet.\n",
    "#\n",
    "# Here I grab from Turney and Littman 2003.\n",
    "\n",
    "POS_SEEDS = ['good', 'nice', 'excellent', 'positive', 'fortunate', 'correct', 'superior']\n",
    "NEG_SEEDS = ['bad', 'nasty', 'poor', 'negative', 'unfortunate', 'wrong', 'inferior']\n",
    "\n",
    "def get_centroid(word_set):\n",
    "    centroid = np.zeros(NUM_DIM)\n",
    "    ctr = 0.\n",
    "    for word in word_set:\n",
    "        if word in glove:\n",
    "            centroid += glove[word]\n",
    "            ctr += 1.\n",
    "    return centroid / ctr\n",
    "\n",
    "\n",
    "POSITIVE_CENTROID = get_centroid(POS_SEEDS)\n",
    "NEGATIVE_CENTROID = get_centroid(NEG_SEEDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get a sense of the space\n",
    "print \"Difference between centroids:\"\n",
    "print POSITIVE_CENTROID - NEGATIVE_CENTROID\n",
    "print\n",
    "print \"Distance between centroids:\"\n",
    "print cosine(POSITIVE_CENTROID, NEGATIVE_CENTROID)\n",
    "print\n",
    "print \"Average distance positive seeds -> positive centroid\"\n",
    "print np.mean([cosine(POSITIVE_CENTROID, glove[word]) for word in POS_SEEDS])\n",
    "print \"Average distance positive seeds -> negative centroid\"\n",
    "print np.mean([cosine(NEGATIVE_CENTROID, glove[word]) for word in POS_SEEDS])\n",
    "print\n",
    "print \"Average distance negative seeds -> positive centroid\"\n",
    "print np.mean([cosine(POSITIVE_CENTROID, glove[word]) for word in NEG_SEEDS])\n",
    "print \"Average distance negative seeds -> negative centroid\"\n",
    "print np.mean([cosine(NEGATIVE_CENTROID, glove[word]) for word in NEG_SEEDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want a dataset in which we have clever features that are\n",
    "# based on the additional information we get from GloVe.\n",
    "\n",
    "TOTAL_FEATURES = 10\n",
    "\n",
    "def convert_to_vector(paragraph):\n",
    "    unigrams = paragraph.split()\n",
    "    \n",
    "    num_out_of_vocab = 0\n",
    "    dist_to_positive = np.zeros(len(unigrams))\n",
    "    dist_to_negative = np.zeros(len(unigrams))\n",
    "    for i, word in enumerate(unigrams):\n",
    "        if word in glove:\n",
    "            dist_to_positive[i-num_out_of_vocab] = cosine(POSITIVE_CENTROID, glove[word])\n",
    "            dist_to_negative[i-num_out_of_vocab] = cosine(NEGATIVE_CENTROID, glove[word])\n",
    "        else:\n",
    "            num_out_of_vocab += 1\n",
    "            \n",
    "    # Cut out any unused words\n",
    "    if num_out_of_vocab > 0:\n",
    "        dist_to_positive = dist_to_positive[:-num_out_of_vocab]\n",
    "        dist_to_negative = dist_to_negative[:-num_out_of_vocab]\n",
    "        \n",
    "    # Sort them so we can take the top-n easily\n",
    "    dist_to_positive = sorted(dist_to_positive)\n",
    "    dist_to_negative = sorted(dist_to_negative)\n",
    "            \n",
    "    # Derive a feature representation\n",
    "    representation = np.zeros(TOTAL_FEATURES)\n",
    "    # First feature: How close is this paragraph on average\n",
    "    # to the centroid of \"positive\" words?\n",
    "    representation[0] = np.mean(dist_to_positive)\n",
    "\n",
    "    # Second feature: Same for \"negative\" words\n",
    "    representation[1] = np.mean(dist_to_negative)\n",
    "\n",
    "    # Third feature: What's the average of the top 3 words\n",
    "    # for positive?\n",
    "    representation[2] = np.mean(dist_to_positive[:3])\n",
    "\n",
    "    # Fourth feature: What's the average of the top 3 words\n",
    "    # for negative?\n",
    "    representation[3] = np.mean(dist_to_negative[:3])\n",
    "\n",
    "    # Fifth feature: Top 1 positive word?\n",
    "    representation[4] = dist_to_positive[0]\n",
    "\n",
    "    # Sixth feature: Top 1 negative word?\n",
    "    representation[5] = dist_to_negative[0]\n",
    "\n",
    "    # Seventh feature: What percent of words are out of vocab?\n",
    "    representation[6] = num_out_of_vocab*1. / len(unigrams)\n",
    "    \n",
    "    # Eighth feature: What's the difference between the words'\n",
    "    # distances to the positive seed centroid vs. the negative\n",
    "    # seed centroid?\n",
    "    representation[7] = np.sum(dist_to_positive) - np.sum(dist_to_negative)\n",
    "\n",
    "    # Ninth feature: What's the difference between the words'\n",
    "    # closest distance to the positive seed set vs. the negative\n",
    "    # seed set?\n",
    "    if word in glove: #production code should check that the seed words are in glove\n",
    "        positive_distances = np.min([cosine(glove[word], glove[pos]) for pos in POS_SEEDS])\n",
    "        negative_distances = np.min([cosine(glove[word], glove[neg]) for neg in NEG_SEEDS])\n",
    "        representation[8] = positive_distances - negative_distances\n",
    "        \n",
    "    # Tenth feature: What's the difference between the words'\n",
    "    # closest distance to the positive seed set vs. the negative\n",
    "    # seed set?\n",
    "    # This is \"semantic orientation\"\n",
    "    if word in glove: #production code should check that the seed words are in glove\n",
    "        positive_distances = np.sum([cosine(glove[word], glove[pos]) for pos in POS_SEEDS])\n",
    "        negative_distances = np.sum([cosine(glove[word], glove[neg]) for neg in NEG_SEEDS])\n",
    "        representation[9] = positive_distances - negative_distances\n",
    "\n",
    "    return representation\n",
    "\n",
    "convert_to_vector(\"this is an awful dfsd sentence .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_dataset(dataset):\n",
    "    dataset_matrix = np.zeros((len(dataset), TOTAL_FEATURES))\n",
    "    for i,paragraph in enumerate(dataset):\n",
    "        dataset_matrix[i] = convert_to_vector(paragraph)\n",
    "    return dataset_matrix\n",
    "\n",
    "X_train_vector = convert_dataset(X_train)\n",
    "print X_train_vector.shape\n",
    "X_dev_vector = convert_dataset(X_dev)\n",
    "print X_dev_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "clf.fit(X_train_vector, y_train)\n",
    "y_dev_hat = clf.predict(X_dev_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate\n",
    "# No cross-validation this round, but we can use that in the \n",
    "# future to get a sense of the variability of the method\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy:\"\n",
    "print metrics.accuracy_score(y_dev, y_dev_hat)\n",
    "\n",
    "print\n",
    "\n",
    "print \"Classification metrics:\"\n",
    "print metrics.classification_report(y_dev, y_dev_hat)\n",
    "\n",
    "print \n",
    "\n",
    "print \"Confusion matrix:\"\n",
    "print \"(Rows are truth, columns are predictions)\"\n",
    "print metrics.confusion_matrix(y_dev, y_dev_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's scale the data\n",
    "\n",
    "It's often a good idea to force the data to have a mean of 0 and variance of 1.\n",
    "\n",
    "But remember, we can only using the training data to impose this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train_vector)\n",
    "X_train_vector_s = scaler.transform(X_train_vector)\n",
    "X_dev_vector_s = scaler.transform(X_dev_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rev_clf = linear_model.LogisticRegression()\n",
    "rev_clf.fit(X_train_vector_s, y_train)\n",
    "y_dev_hat = rev_clf.predict(X_dev_vector_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate\n",
    "# No cross-validation this round, but we can use that in the \n",
    "# future to get a sense of the variability of the method\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy:\"\n",
    "print metrics.accuracy_score(y_dev, y_dev_hat)\n",
    "\n",
    "print\n",
    "\n",
    "print \"Classification metrics:\"\n",
    "print metrics.classification_report(y_dev, y_dev_hat)\n",
    "\n",
    "print \n",
    "\n",
    "print \"Confusion matrix:\"\n",
    "print \"(Rows are truth, columns are predictions)\"\n",
    "print metrics.confusion_matrix(y_dev, y_dev_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we do get a gain.\n",
    "\n",
    "We're doing pretty good for only 10 features -- and since a lot of these features are probably correlated with each other, we could probably do just about this good with even fewer features.\n",
    "\n",
    "But really, we would probably want to combine this information with the unigram model that was performing best earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see which features are most helpful\n",
    "\n",
    "We can look at the coefficients on our model to get a sense for which features matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rev_clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [\"How close to the positive centroid on average?\",\n",
    "\"How close to the negative centroid on average?\",\n",
    "\"How close are the top 3 words to the positive centroid?\",\n",
    "\"How close are the top 3 words to the negative centroid?\",\n",
    "\"How close is the closest word to the positive centroid?\",\n",
    "\"How close is the closest word to the negative centroid?\",\n",
    "\"What percent of words are out of vocabulary?\",\n",
    "\"What's the difference between dist to positive centroid & negative centroid?\",\n",
    "\"What's the difference between closest dist to positive vs. negative word?\",\n",
    "\"What's the semantic orientation?\"]\n",
    "\n",
    "data = zip(labels, rev_clf.coef_[0])\n",
    "data_sorted = sorted(data, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print \"From most important to least important feature:\"\n",
    "for label, coef in data_sorted:\n",
    "    print \"%5.2f %s\" % (coef, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sign interpretation here is a bit complex.\n",
    "\n",
    "Big values for distance mean \"low probability of being related\".  So a negative score on a positive centroid means that lower values are good for getting the positive class.  Similarly, positive scores on the negative centroids mean that higher distances are good for getting the positive class.\n",
    "\n",
    "We find from this that larger features are more stable.  One-offs aren't as useful in this context as the sentiment of the sentence as a whole."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
